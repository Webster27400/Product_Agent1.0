import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.groq import Groq
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool, ToolMetadata, FunctionTool
import os
from datetime import datetime

# --- Konfiguracja klucza API ---
# Wczytujemy klucz z menedÅ¼era sekretÃ³w Streamlit
if "GROQ_API_KEY" in st.secrets:
    os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]
else:
    st.error("Nie znaleziono klucza GROQ_API_KEY w sekretach. Dodaj go w ustawieniach wdroÅ¼enia.")
    st.stop()

# --- Interfejs Aplikacji Streamlit ---
st.title("ğŸ’¡ Agent Geneza (Wersja Finalna)")
st.markdown("Rozpocznij rozmowÄ™ z agentem.")

# âœ… FunkcjonalnoÅ›Ä‡: PrzeÅ‚Ä…cznik jÄ™zyka w panelu bocznym
language = st.sidebar.radio(
    "Wybierz jÄ™zyk odpowiedzi:",
    ('Polski', 'Angielski')
)

# âœ… FunkcjonalnoÅ›Ä‡: Dynamiczne instrukcje dla AI
prompt_pl = "JesteÅ› proaktywnym asystentem menedÅ¼era produktu. Twoim celem jest nie tylko odpowiadaÄ‡ na pytania, ale takÅ¼e identyfikowaÄ‡ potencjalne ryzyka i szanse. Odpowiadaj zawsze po polsku, w przyjaznym, ale profesjonalnym tonie."
prompt_en = "You are a proactive assistant to a product manager. Your goal is not only to answer questions but also to identify potential risks and opportunities. Always respond in English, in a friendly yet professional tone."

system_prompt = prompt_pl if language == 'Polski' else prompt_en

# âœ… FunkcjonalnoÅ›Ä‡: PoÅ‚Ä…czenie z potÄ™Å¼nym modelem AI (Groq) z dynamicznÄ… instrukcjÄ…
Settings.llm = Groq(model="llama3-8b-8192", system_prompt=system_prompt)
# âœ… FunkcjonalnoÅ›Ä‡: UÅ¼ycie lokalnego modelu do "rozumienia" tekstu
Settings.embed_model = "local:BAAI/bge-small-en-v1.5"

# âœ… FunkcjonalnoÅ›Ä‡: Optymalizacja - Wczytywanie danych tylko raz
@st.cache_resource
def load_index():
    with st.spinner("WczytujÄ™ i indeksujÄ™ dane (tylko raz)..."):
        reader = SimpleDirectoryReader(input_dir=".")
        documents = reader.load_data()
        index = VectorStoreIndex.from_documents(documents)
        return index

index = load_index()

# Tworzymy silnik zapytaÅ„ z aktualnym modelem LLM
query_engine = index.as_query_engine(llm=Settings.llm)

# âœ… FunkcjonalnoÅ›Ä‡: Dwa rÃ³Å¼ne narzÄ™dzia dla agenta
# NarzÄ™dzie 1: Data
def get_todays_date(fake_arg: str = "") -> str:
    """Zwraca dzisiejszÄ… datÄ™ w formacie ROK-MIESIÄ„C-DZIEÅƒ."""
    return datetime.now().strftime("%Y-%m-%d")

date_tool = FunctionTool.from_defaults(fn=get_todays_date, name="narzedzie_daty", description="To narzÄ™dzie zwraca dzisiejszÄ… datÄ™.")

# NarzÄ™dzie 2: Dokumenty
document_tool = QueryEngineTool(
    query_engine=query_engine,
    metadata=ToolMetadata(
        name="analizator_dokumentow_klientow",
        description=(
            "To narzÄ™dzie sÅ‚uÅ¼y do analizy i podsumowywania dokumentÃ³w zawierajÄ…cych opinie i zgÅ‚oszenia od klientÃ³w. "
            "UÅ¼yj go, jeÅ›li pytanie dotyczy feedbacku, bugÃ³w, prÃ³Å›b o nowe funkcje, notatek ze spotkaÅ„, plikÃ³w CSV lub podsumowania informacji."
        ),
    ),
)

# âœ… FunkcjonalnoÅ›Ä‡: Zaawansowany agent typu ReAct, ktÃ³ry decyduje, ktÃ³rego narzÄ™dzia uÅ¼yÄ‡
agent = ReActAgent.from_tools(tools=[date_tool, document_tool], llm=Settings.llm, verbose=True)

# âœ… FunkcjonalnoÅ›Ä‡: Interfejs w stylu czatu z historiÄ… rozmowy
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Twoje pytanie:"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Agent myÅ›li... ğŸ¤”"):
            response = agent.chat(prompt)
            st.write(str(response))
            st.session_state.messages.append({"role": "assistant", "content": str(response)})