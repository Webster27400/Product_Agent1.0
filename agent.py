import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.groq import Groq
import os

# --- Konfiguracja Agenta ---
# BEZPIECZNY SPOS√ìB: Wczytujemy klucz z mened≈ºera sekret√≥w Streamlit
# Ten kod bƒôdzie dzia≈Ça≈Ç zar√≥wno lokalnie, jak i po wdro≈ºeniu w chmurze
os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]

# Ustawiamy modele
Settings.llm = Groq(
    model="llama3-8b-8192",
    system_prompt="Jeste≈õ proaktywnym asystentem mened≈ºera produktu. Odpowiadaj zawsze po polsku."
)
Settings.embed_model = "local:BAAI/bge-small-en-v1.5"

# --- Interfejs Aplikacji Streamlit ---
st.title("üí° Agent Geneza") # Zmieniamy nazwƒô, tak jak chcia≈Çe≈õ :)
st.markdown("Zadaj pytanie dotyczƒÖce danych z plik√≥w `data.csv` i `notatki.txt`.")

@st.cache_resource
def load_index():
    with st.spinner("Wczytujƒô i indeksujƒô dane (tylko raz)..."):
        reader = SimpleDirectoryReader(input_dir=".")
        documents = reader.load_data()
        index = VectorStoreIndex.from_documents(documents)
        return index.as_query_engine()

query_engine = load_index()

user_question = st.text_input("Twoje pytanie:")

if user_question:
    with st.spinner("Agent my≈õli... ü§î"):
        response = query_engine.query(user_question)
        st.markdown("### Odpowied≈∫ Agenta:")
        st.write(str(response))